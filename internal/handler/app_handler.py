#!/user/bin/env python
# -*- coding: utf-8 -*-
"""
@Time    : 15.4.25 PM11:20
@Author  : 1685821150@qq.com
@File    : app_handler.py
"""
import json
import os
import uuid
from dataclasses import dataclass
from operator import itemgetter
from queue import Queue
from threading import Thread
from typing import Literal, Annotated, Generator

from flask import request
from injector import inject
from langchain.memory import ConversationBufferWindowMemory
from langchain_community.chat_message_histories import FileChatMessageHistory
from langchain_core.messages import ToolMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, \
    MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState
from redis import Redis
from langgraph.constants import END

from internal.core.agent.agents.function_call_agent import FunctionCallAgent
from internal.core.agent.entities.agent_entity import AgentConfig
from internal.core.tools.builtin_tools.providers.builtin_provider_manager import BuiltinProviderManager
from internal.schema.app_schema import CompletionReq
from internal.service import AppService
from internal.service.conversation_service import ConversationService
from internal.service.embeddings_service import EmbeddingsService
from internal.task.demo_task import demo_task
from pkg.response import success_json, validate_error_json, success_message
from pkg.response.response import compact_generate_response


@inject
@dataclass
class AppHandler:
    """åº”ç”¨æ§åˆ¶å™¨"""

    app_service: AppService
    redis_client: Redis
    embeddings_service: EmbeddingsService
    builtin_provider_manager: BuiltinProviderManager
    conversation_service: ConversationService

    def debug(self, app_id):
        req = CompletionReq()
        if not req.validate():
            return validate_error_json(req.errors)
        # 1. åˆ›å»ºprompt
        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template("ä½ æ˜¯openaiå¼€å‘çš„èŠå¤©æœºå™¨äººï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜"),
            MessagesPlaceholder("history"),
            HumanMessagePromptTemplate.from_template("{query}")
        ])
        # 2. åˆ›å»ºllmå¤§è¯­è¨€æ¨¡å‹
        llm = ChatOpenAI(model="moonshot-v1-8k")
        # 3. åˆ›å»ºè®°å¿†
        memory = ConversationBufferWindowMemory(
            k=3,
            input_key="query",
            output_key="output",
            return_messages=True,
            chat_memory=FileChatMessageHistory("./storage/memory/chat_history.txt")
        )
        # 4. åˆ›å»ºé“¾
        chain = RunnablePassthrough.assign(
            history=RunnableLambda(memory.load_memory_variables) | itemgetter("history")
        ) | prompt | llm | StrOutputParser()

        chain_input = {"query": req.query.data}
        content = chain.invoke(chain_input)
        memory.save_context(chain_input, outputs={"output": content})
        return success_json({"content": content})

    def create_app(self):
        app = self.app_service.create_app()
        return success_message(f"åº”ç”¨åˆ›å»ºæˆåŠŸï¼Œidä¸º{app.id}")

    def get_app(self, id: uuid.UUID):
        app = self.app_service.get_app(id)
        return success_message(f"åº”ç”¨å·²ç»æˆåŠŸè·å–ï¼Œåå­—æ˜¯{app.name}")

    def update_app(self, id: uuid.UUID):
        app = self.app_service.update_app(id)
        return success_message(f"åº”ç”¨åç§°ä¿®æ”¹æˆåŠŸï¼Œä¿®æ”¹åçš„åå­—æ˜¯{app.name}")

    def delete_app(self, id: uuid.UUID):
        app = self.app_service.delete_app(id)
        return success_message(f"åº”ç”¨å·²ç»æˆåŠŸåˆ é™¤ï¼Œidä¸º:{app.id}")

    def completion(self):
        """èŠå¤©æ¥å£"""
        # 1.æå–ä»æ¥å£ä¸­è·å–çš„è¾“å…¥ï¼ŒPOST
        json_data = request.get_json()
        req = CompletionReq(data=json_data)
        if not req.validate():
            return validate_error_json(req.errors)

        # 2.æ„å»ºOpenAIå®¢æˆ·ç«¯ï¼Œå¹¶å‘èµ·è¯·æ±‚
        # client = OpenAI(
        #     api_key=os.getenv('OPENAI_API_KEY'),
        #     base_url=os.getenv('OPENAI_BASE_URL'),
        # )
        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template("ä½ æ˜¯OpenAIå¼€å‘çš„èŠå¤©æœºå™¨äººï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„è¾“å…¥å›å¤å¯¹åº”çš„ä¿¡æ¯"),
            HumanMessagePromptTemplate.from_template("{query}")
        ])

        llm = ChatOpenAI(model="moonshot-v1-8k")

        parser = StrOutputParser()

        content = prompt | llm | parser
        # completion = client.chat.completions.create(
        #     model="moonshot-v1-8k",
        #     messages=[
        #         {"role": "system",
        #          "content": "ä½ æ˜¯OpenAIå¼€å‘çš„èŠå¤©æœºå™¨äººï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„è¾“å…¥å›å¤å¯¹åº”çš„ä¿¡æ¯"},
        #         {"role": "user", "content": req.query.data}
        #     ],
        #     temperature=0.3,
        # )
        return success_json({"content": content.invoke(json_data['query'])})

    def ping(self):
        agent = FunctionCallAgent(AgentConfig(
            llm=ChatOpenAI(model=os.getenv("BASE_CHAT_MODEL")),
            preset_prompt="ä½ æ˜¯ä¸€ä¸ªæ‹¥æœ‰20å¹´ç»éªŒçš„è¯—äººï¼Œè¯·æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸»é¢˜å†™ä¸€é¦–è¯—"
        ))
        state = agent.run("ç¨‹åºå‘˜", [], "")
        content = state['messages'][-1]['content']
        return success_json({"content": content})

    def _ping(self):
        # self.redis_client.set('name', 'zhangsan')
        # print(self.redis_client.get("name"))
        # result = demo_task.delay(uuid.uuid4())
        # value = {
        #     "token_count": self.embeddings_service.calculate_token_count("ä½ å¥½ï¼Œä½ æ˜¯è°"),
        #     # "embedding_value": self.embeddings_service.embeddings.embed_query("ä½ å¥½ï¼Œä½ æ˜¯è°")
        # }
        value = self.conversation_service.generate_suggested_questions("""
        LLM å°±æ˜¯ä¸€ç§é€šè¿‡è®­ç»ƒå¤§é‡æ–‡æœ¬æ•°æ®ã€èƒ½ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ï¼ˆç”šè‡³ä»£ç ï¼‰çš„äººå·¥æ™ºèƒ½æ¨¡å‹ã€‚ä½ ç°åœ¨åœ¨ç”¨çš„ ChatGPT å°±æ˜¯åŸºäº LLM çš„äº§å“ä¹‹ä¸€ã€‚

        ğŸ§  LLM èƒ½åšä»€ä¹ˆï¼Ÿ
        å®ƒå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š
        
        è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆï¼ˆå†™æ–‡ç« ã€æ‘˜è¦ã€æ”¹å†™ã€å¯¹è¯ç­‰ï¼‰
        
        ä»£ç ç”Ÿæˆä¸è°ƒè¯•ï¼ˆå¦‚ Pythonã€JavaScriptã€Java ç­‰ï¼‰
        
        é—®ç­”ç³»ç»Ÿï¼ˆåƒ ChatGPTã€Claudeã€æ–‡å¿ƒä¸€è¨€ï¼‰
        
        ç¿»è¯‘ã€å¤šè¯­è¨€æ”¯æŒ
        
        ä¿¡æ¯æŠ½å–ï¼ˆä»æ–‡æ¡£ä¸­æå–å…³é”®å­—æ®µï¼‰
        
        å¤šæ¨¡æ€èƒ½åŠ›ï¼ˆå›¾åƒ + æ–‡æœ¬ï¼Œå¦‚ GPT-4oï¼‰
        
        ğŸ”§ LLM çš„å·¥ä½œåŸç†ï¼ˆç®€ç•¥ç‰ˆï¼‰ï¼š
        é¢„è®­ç»ƒï¼ˆPretrainingï¼‰ï¼šåœ¨å¤§é‡æ–‡æœ¬ï¼ˆå¦‚ Wikipediaã€ä¹¦ç±ã€ç½‘é¡µï¼‰ä¸Šè¿›è¡Œè¯­è¨€å»ºæ¨¡ã€‚
        
        å¾®è°ƒï¼ˆFine-tuningï¼‰ï¼šæ ¹æ®å…·ä½“ä»»åŠ¡ï¼ˆå¦‚é—®ç­”ã€èŠå¤©ï¼‰è¿›ä¸€æ­¥è®­ç»ƒã€‚
        
        æ¨ç†ï¼ˆInferenceï¼‰ï¼šç”¨æˆ·è¾“å…¥ä¸€å¥è¯ï¼Œæ¨¡å‹åŸºäºä¸Šä¸‹æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªæœ€åˆç†çš„è¯ï¼Œä¸€æ­¥æ­¥ç”Ÿæˆå›ç­”ã€‚""")
        return success_json({"ping": value})

    def debug(self, app_id: uuid.UUID):
        req = CompletionReq()
        if not req.validate():
            return validate_error_json(req.errors)

        # 2. åˆ›å»ºé˜Ÿåˆ—å¹¶æå–queryæ•°æ®
        q = Queue()
        query = req.query.data

        # 3. åˆ›å»ºgraphå›¾åº”ç”¨ç¨‹åº
        def graph_app() -> None:
            """åˆ›å»ºGraphå›¾ç¨‹åºåº”ç”¨å¹¶æ‰§è¡Œ"""
            # 3.1 åˆ›å»ºtoolså·¥å…·åˆ—è¡¨
            tools = [
                self.builtin_provider_manager.get_tool('google', "google_serper")(),
                self.builtin_provider_manager.get_tool('gaode', "gaode_weather")(),
                self.builtin_provider_manager.get_tool('dalle', "dalle3")(),
            ]

            # 3.2 å®šä¹‰å¤§è¯­è¨€æ¨¡å‹ã€èŠå¤©æœºå™¨äººèŠ‚ç‚¹
            def chatbot(state: MessagesState) -> MessagesState:
                """æœºå™¨äººèŠå¤©èŠ‚ç‚¹"""
                # 3.2.1åˆ›å»ºLLMå¤§è¯­è¨€æ¨¡å‹
                llm = ChatOpenAI(model=os.getenv("BASE_CHAT_MODEL"), temperature=0.7).bind_tools(tools)

                # 3.2.2 è°ƒç”¨stream()å‡½æ•°è·å–æµå¼è¾“å‡ºå†…å®¹ï¼Œå¹¶åˆ¤æ–­ç”Ÿæˆå†…å®¹æ˜¯æ–‡æœ¬è¿˜æ˜¯å·¥å…·è°ƒç”¨å‡½æ•°
                is_first_chunk = True
                is_tool_call = False
                gathered = None
                id = str(uuid.uuid4())
                for chunk in llm.stream(state['messages']):
                    # 3.2.3æ£€æµ‹æ˜¯ä¸æ˜¯ç¬¬ä¸€ä¸ªå—ï¼Œéƒ¨åˆ†LLMçš„ç¬¬ä¸€ä¸ªå—ä¸ä¼šè¾“å‡ºå†…å®¹ï¼Œéœ€è¦æŠ›å¼ƒæ‰
                    if is_first_chunk and chunk.content == "" and not chunk.tool_calls:
                        continue

                    # 3.2.4 å åŠ ç›¸åº”çš„åŒºå—
                    if is_first_chunk:
                        gathered = chunk
                        print(chunk, 'chunkæ˜¯è¿™ä¸ª')
                        is_first_chunk = False
                    else:
                        gathered += chunk

                    # 3.2.5 åˆ¤æ–­æ˜¯å·¥å…·è°ƒç”¨è¿˜æ˜¯æ–‡æœ¬ç”Ÿæˆï¼Œå¾€é˜Ÿåˆ—ä¸­æ·»åŠ ä¸åŒçš„æ•°æ®
                    if chunk.tool_calls or is_tool_call:
                        is_tool_call = True
                        q.put({
                            "id": id,
                            "event": "agent_thought",
                            "data": json.dumps(chunk.tool_call_chunks)
                        })
                    else:
                        q.put({
                            "id": id,
                            "event": "agent_message",
                            "data": json.dumps(chunk.content)
                        })
                return {"messages": [gathered]}

            # 3.3 å®šä¹‰å·¥å…·ã€å‡½æ•°è°ƒç”¨èŠ‚ç‚¹
            def tool_executor(state: MessagesState) -> MessagesState:
                # å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
                # 3.3.1 æå–æ•°æ®çŠ¶æ€ä¸­çš„tool_calls
                tool_calls = state['messages'][-1].tool_calls

                # 3.3.2 å°†å·¥å…·åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ä¾¿äºä½¿ç”¨
                tools_by_name = {tool.name: tool for tool in tools}

                # 3.3.3 æ‰§è¡Œå·¥å…·å¹¶å¾—åˆ°å¯¹åº”ç»“æœ
                messages = []
                for tool_call in tool_calls:
                    id = str(uuid.uuid4())
                    tool = tools_by_name[tool_call['name']]
                    tool_result = tool.invoke(tool_call['args'])
                    print(tool_result, 'tool_result====')
                    messages.append(ToolMessage(
                        tool_call_id=tool_call['id'],
                        content=json.dumps(tool_result),
                        name=tool_call['name']
                    ))
                    q.put({
                        "id": id,
                        "event": "agent_action",
                        "data": json.dumps(tool_result)
                    })
                return {"messages": messages}

            # 3.4å®šä¹‰è·¯ç”±å‡½æ•°
            def route(state: MessagesState) -> Literal["tool_executor", "__end__"]:
                """å®šä¹‰è·¯ç”±èŠ‚ç‚¹ï¼Œç¡®è®¤ä¸‹ä¸€æ­¥æ­¥éª¤"""
                ai_message = state["messages"][-1]
                if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
                    return "tool_executor"
                return END

            # 3.5 åˆ›å»ºçŠ¶æ€å›¾
            graph_builder = StateGraph(MessagesState)

            # 3.6 æ·»åŠ èŠ‚ç‚¹
            graph_builder.add_node("llm", chatbot)
            graph_builder.add_node("tool_executor", tool_executor)

            # 3.7 æ·»åŠ è¾¹
            graph_builder.set_entry_point("llm")
            graph_builder.add_conditional_edges("llm", route)
            graph_builder.add_edge("tool_executor", "llm")

            # 3.8ç¼–è¯‘å›¾ç¨‹åºä¸ºå¯è¿è¡Œç»„ä»¶
            graph = graph_builder.compile()

            # 3.9 è°ƒç”¨å›¾ç»“æ„ç¨‹åºå¹¶è·å–ç»“æœ
            result = graph.invoke({"messages": [("human", query)]})
            print("æœ€ç»ˆç»“æœ: ", result)
            q.put(None)

        def stream_event_response() -> Generator:
            """æµå¼äº‹ä»¶è¾“å‡ºå“åº”"""
            # 1. ä»é˜Ÿåˆ—ä¸­è·å–æ•°æ®å¹¶ä½¿ç”¨yieldæŠ›å‡º
            while True:
                item = q.get()
                if item is None:
                    break
                # 2. ä½¿ç”¨yieldå…³é”®å­—è¿”å›å¯¹åº”çš„æ•°æ®
                yield f"event: {item.get('event')}\ndata: {json.dumps(item)}\n\n"
                q.task_done()

        t = Thread(target=graph_app)
        t.start()

        return compact_generate_response(stream_event_response())
