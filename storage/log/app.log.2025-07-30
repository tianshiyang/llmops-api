[2025-07-30 20:10:17,590.590] _internal.py -> _log line:97 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
[2025-07-30 20:10:17,591.591] _internal.py -> _log line:97 [INFO]: [33mPress CTRL+C to quit[0m
[2025-07-30 20:10:17,591.591] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 20:10:19,243.243] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 20:10:19,252.252] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:17:24,817.817] _internal.py -> _log line:97 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
[2025-07-30 23:17:24,817.817] _internal.py -> _log line:97 [INFO]: [33mPress CTRL+C to quit[0m
[2025-07-30 23:17:24,818.818] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 23:17:26,377.377] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 23:17:26,394.394] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:17:37,295.295] http.py -> _register_error_handler line:70 [ERROR]: An error occurred: with_structured_output is not implemented for this model.
Traceback (most recent call last):
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py", line 140, in ping
    value = self.conversation_service.generate_conversation_name("""
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py", line 59, in generate_conversation_name
    structured_llm = llm.with_structured_output(ConversationInfo)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1482, in with_structured_output
    raise NotImplementedError(msg)
NotImplementedError: with_structured_output is not implemented for this model.
[2025-07-30 23:17:37,300.300] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:17:37] "[35m[1mGET /ping HTTP/1.1[0m" 500 -
[2025-07-30 23:18:35,145.145] _internal.py -> _log line:97 [INFO]:  * Detected change in '/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py', reloading
[2025-07-30 23:18:35,471.471] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 23:18:37,118.118] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 23:18:37,128.128] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:18:39,454.454] http.py -> _register_error_handler line:70 [ERROR]: An error occurred: 1 validation error for ChatOpenAI
model
  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py", line 140, in ping
    value = self.conversation_service.generate_conversation_name("""
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py", line 58, in generate_conversation_name
    llm = ChatOpenAI(model=os.getenv("base_chat_model"), temperature=0)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for ChatOpenAI
model
  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
[2025-07-30 23:18:39,460.460] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:18:39] "[35m[1mGET /ping HTTP/1.1[0m" 500 -
[2025-07-30 23:19:08,388.388] _internal.py -> _log line:97 [INFO]:  * Detected change in '/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py', reloading
[2025-07-30 23:19:08,693.693] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 23:19:10,307.307] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 23:19:10,316.316] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:19:12,787.787] http.py -> _register_error_handler line:70 [ERROR]: An error occurred: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support this tool_choice', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
Traceback (most recent call last):
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py", line 140, in ping
    value = self.conversation_service.generate_conversation_name("""
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py", line 70, in generate_conversation_name
    conversation_info = chain.invoke({"query": query})
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3046, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5434, in invoke
    return self.bound.invoke(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 980, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 799, in generate
    self._generate_with_cache(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1045, in _generate_with_cache
    result = self._generate(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 995, in _generate
    response = self.client.create(**payload)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", line 925, in create
    return self._post(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support this tool_choice', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
[2025-07-30 23:19:12,793.793] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:19:12] "[35m[1mGET /ping HTTP/1.1[0m" 500 -
[2025-07-30 23:19:58,399.399] _internal.py -> _log line:97 [INFO]:  * Detected change in '/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py', reloading
[2025-07-30 23:19:58,674.674] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 23:20:00,313.313] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 23:20:00,410.410] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:20:03,626.626] http.py -> _register_error_handler line:70 [ERROR]: An error occurred: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support this tool_choice', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
Traceback (most recent call last):
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py", line 140, in ping
    value = self.conversation_service.generate_conversation_name("""
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py", line 70, in generate_conversation_name
    conversation_info = chain.invoke({"query": query})
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3046, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5434, in invoke
    return self.bound.invoke(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 980, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 799, in generate
    self._generate_with_cache(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1045, in _generate_with_cache
    result = self._generate(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 995, in _generate
    response = self.client.create(**payload)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", line 925, in create
    return self._post(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support this tool_choice', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
[2025-07-30 23:20:03,633.633] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:20:03] "[35m[1mGET /ping HTTP/1.1[0m" 500 -
[2025-07-30 23:32:14,963.963] _internal.py -> _log line:97 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
[2025-07-30 23:32:14,964.964] _internal.py -> _log line:97 [INFO]: [33mPress CTRL+C to quit[0m
[2025-07-30 23:32:14,964.964] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 23:32:16,460.460] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 23:32:16,470.470] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:32:26,636.636] http.py -> _register_error_handler line:70 [ERROR]: An error occurred: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
Traceback (most recent call last):
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py", line 140, in ping
    value = self.conversation_service.generate_conversation_name("""
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py", line 70, in generate_conversation_name
    conversation_info = chain.invoke({"query": query})
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3046, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5434, in invoke
    return self.bound.invoke(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 980, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 799, in generate
    self._generate_with_cache(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1045, in _generate_with_cache
    result = self._generate(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 995, in _generate
    response = self.client.create(**payload)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", line 925, in create
    return self._post(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
[2025-07-30 23:32:26,647.647] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:32:26] "[35m[1mGET /ping HTTP/1.1[0m" 500 -
[2025-07-30 23:32:49,535.535] _internal.py -> _log line:97 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
[2025-07-30 23:32:49,536.536] _internal.py -> _log line:97 [INFO]: [33mPress CTRL+C to quit[0m
[2025-07-30 23:32:49,536.536] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 23:32:50,948.948] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 23:32:50,961.961] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:32:54,830.830] http.py -> _register_error_handler line:70 [ERROR]: An error occurred: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support this tool_choice', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
Traceback (most recent call last):
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py", line 140, in ping
    value = self.conversation_service.generate_conversation_name("""
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py", line 70, in generate_conversation_name
    conversation_info = chain.invoke({"query": query})
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3046, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5434, in invoke
    return self.bound.invoke(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 980, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 799, in generate
    self._generate_with_cache(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1045, in _generate_with_cache
    result = self._generate(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 995, in _generate
    response = self.client.create(**payload)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", line 925, in create
    return self._post(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support this tool_choice', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
[2025-07-30 23:32:54,842.842] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:32:54] "[35m[1mGET /ping HTTP/1.1[0m" 500 -
[2025-07-30 23:34:03,866.866] _internal.py -> _log line:97 [INFO]:  * Detected change in '/Users/icourt1/Desktop/code/llmops/llmops-api/internal/entity/conversation_entity.py', reloading
[2025-07-30 23:34:04,170.170] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 23:34:05,886.886] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 23:34:05,896.896] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:34:09,332.332] http.py -> _register_error_handler line:70 [ERROR]: An error occurred: Error code: 400 - {'error': {'message': 'This response_format type is unavailable now', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
Traceback (most recent call last):
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py", line 140, in ping
    value = self.conversation_service.generate_conversation_name("""
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py", line 70, in generate_conversation_name
    conversation_info = chain.invoke({"query": query})
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3046, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5434, in invoke
    return self.bound.invoke(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 980, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 799, in generate
    self._generate_with_cache(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1045, in _generate_with_cache
    result = self._generate(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 973, in _generate
    _handle_openai_bad_request(e)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 971, in _generate
    response = self.root_client.beta.chat.completions.parse(**payload)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/resources/beta/chat/completions.py", line 158, in parse
    return self._post(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'This response_format type is unavailable now', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
[2025-07-30 23:34:09,347.347] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:34:09] "[35m[1mGET /ping HTTP/1.1[0m" 500 -
[2025-07-30 23:36:45,003.003] http.py -> _register_error_handler line:70 [ERROR]: An error occurred: Error code: 400 - {'error': {'message': 'This response_format type is unavailable now', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
Traceback (most recent call last):
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py", line 140, in ping
    value = self.conversation_service.generate_conversation_name("""
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py", line 70, in generate_conversation_name
    conversation_info = chain.invoke({"query": query})
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3046, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5434, in invoke
    return self.bound.invoke(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 980, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 799, in generate
    self._generate_with_cache(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1045, in _generate_with_cache
    result = self._generate(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 973, in _generate
    _handle_openai_bad_request(e)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 971, in _generate
    response = self.root_client.beta.chat.completions.parse(**payload)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/resources/beta/chat/completions.py", line 158, in parse
    return self._post(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'This response_format type is unavailable now', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
[2025-07-30 23:36:45,012.012] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:36:45] "[35m[1mGET /ping HTTP/1.1[0m" 500 -
[2025-07-30 23:36:50,744.744] _internal.py -> _log line:97 [INFO]:  * Detected change in '/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py', reloading
[2025-07-30 23:36:51,058.058] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 23:36:52,808.808] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 23:36:52,817.817] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:37:05,209.209] http.py -> _register_error_handler line:70 [ERROR]: An error occurred: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
Traceback (most recent call last):
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py", line 140, in ping
    value = self.conversation_service.generate_conversation_name("""
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py", line 70, in generate_conversation_name
    conversation_info = chain.invoke({"query": query})
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3046, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5434, in invoke
    return self.bound.invoke(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 980, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 799, in generate
    self._generate_with_cache(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1045, in _generate_with_cache
    result = self._generate(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 973, in _generate
    _handle_openai_bad_request(e)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 971, in _generate
    response = self.root_client.beta.chat.completions.parse(**payload)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/resources/beta/chat/completions.py", line 158, in parse
    return self._post(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
[2025-07-30 23:37:05,216.216] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:37:05] "[35m[1mGET /ping HTTP/1.1[0m" 500 -
[2025-07-30 23:37:42,011.011] _internal.py -> _log line:97 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
[2025-07-30 23:37:42,011.011] _internal.py -> _log line:97 [INFO]: [33mPress CTRL+C to quit[0m
[2025-07-30 23:37:42,011.011] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 23:37:43,437.437] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 23:37:43,450.450] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:37:46,700.700] http.py -> _register_error_handler line:70 [ERROR]: An error occurred: Error code: 404 - {'error': {'message': 'Not found the model https://api.moonshot.cn/v1 or Permission denied', 'type': 'resource_not_found_error'}}
Traceback (most recent call last):
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/flask/app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/handler/app_handler.py", line 140, in ping
    value = self.conversation_service.generate_conversation_name("""
  File "/Users/icourt1/Desktop/code/llmops/llmops-api/internal/service/conversation_service.py", line 70, in generate_conversation_name
    conversation_info = chain.invoke({"query": query})
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3046, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5434, in invoke
    return self.bound.invoke(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 980, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 799, in generate
    self._generate_with_cache(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1045, in _generate_with_cache
    result = self._generate(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py", line 971, in _generate
    response = self.root_client.beta.chat.completions.parse(**payload)
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/resources/beta/chat/completions.py", line 158, in parse
    return self._post(
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/icourt1/.pyenv/versions/llmops-env/lib/python3.10/site-packages/openai/_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'Not found the model https://api.moonshot.cn/v1 or Permission denied', 'type': 'resource_not_found_error'}}
[2025-07-30 23:37:46,708.708] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:37:46] "[35m[1mGET /ping HTTP/1.1[0m" 500 -
[2025-07-30 23:38:46,285.285] _internal.py -> _log line:97 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
[2025-07-30 23:38:46,285.285] _internal.py -> _log line:97 [INFO]: [33mPress CTRL+C to quit[0m
[2025-07-30 23:38:46,286.286] _internal.py -> _log line:97 [INFO]:  * Restarting with stat
[2025-07-30 23:38:47,783.783] _internal.py -> _log line:97 [WARNING]:  * Debugger is active!
[2025-07-30 23:38:47,793.793] _internal.py -> _log line:97 [INFO]:  * Debugger PIN: 682-461-187
[2025-07-30 23:38:53,243.243] _internal.py -> _log line:97 [INFO]: 127.0.0.1 - - [30/Jul/2025 23:38:53] "GET /ping HTTP/1.1" 200 -
